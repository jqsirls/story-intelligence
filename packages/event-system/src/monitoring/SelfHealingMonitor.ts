// Self-Healing Monitor - Enhances existing monitoring with intelligent incident detection and automated recovery\nimport { EventPublisher } from '../EventPublisher';\nimport { EventSubscriber } from '../EventSubscriber';\nimport { MetricsCollector } from './MetricsCollector';\nimport { OpenTelemetryTracer } from './OpenTelemetryTracer';\nimport { Logger } from 'winston';\nimport { CircuitBreaker } from '../CircuitBreaker';\n\nexport interface IncidentDetectionRule {\n  id: string;\n  name: string;\n  description: string;\n  metricName: string;\n  threshold: number;\n  operator: 'gt' | 'lt' | 'eq' | 'gte' | 'lte';\n  windowMinutes: number;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  enabled: boolean;\n  cooldownMinutes: number;\n  autoHealingActions: AutoHealingAction[];\n}\n\nexport interface AutoHealingAction {\n  id: string;\n  type: 'restart_service' | 'scale_up' | 'circuit_breaker' | 'rollback' | 'clear_cache' | 'custom';\n  parameters: Record<string, any>;\n  maxRetries: number;\n  retryDelaySeconds: number;\n  successCriteria: SuccessCriteria;\n}\n\nexport interface SuccessCriteria {\n  metricName: string;\n  expectedValue: number;\n  operator: 'gt' | 'lt' | 'eq' | 'gte' | 'lte';\n  timeoutMinutes: number;\n}\n\nexport interface Incident {\n  id: string;\n  ruleId: string;\n  ruleName: string;\n  severity: string;\n  description: string;\n  detectedAt: Date;\n  resolvedAt?: Date;\n  status: 'open' | 'healing' | 'resolved' | 'failed';\n  metricValue: number;\n  threshold: number;\n  healingActions: HealingActionResult[];\n  context: Record<string, any>;\n}\n\nexport interface HealingActionResult {\n  actionId: string;\n  actionType: string;\n  startedAt: Date;\n  completedAt?: Date;\n  status: 'pending' | 'running' | 'success' | 'failed' | 'timeout';\n  attempts: number;\n  maxRetries: number;\n  error?: string;\n  logs: string[];\n}\n\nexport interface KnowledgeBaseEntry {\n  id: string;\n  incidentPattern: string;\n  symptoms: string[];\n  rootCause: string;\n  solution: string;\n  preventionSteps: string[];\n  successRate: number;\n  lastUpdated: Date;\n  tags: string[];\n}\n\nexport class SelfHealingMonitor {\n  private eventPublisher: EventPublisher;\n  private eventSubscriber: EventSubscriber;\n  private metricsCollector: MetricsCollector;\n  private tracer: OpenTelemetryTracer;\n  private logger: Logger;\n  private detectionRules: Map<string, IncidentDetectionRule> = new Map();\n  private activeIncidents: Map<string, Incident> = new Map();\n  private knowledgeBase: Map<string, KnowledgeBaseEntry> = new Map();\n  private metricHistory: Map<string, Array<{ timestamp: Date; value: number }>> = new Map();\n  private cooldownTracker: Map<string, Date> = new Map();\n  private circuitBreakers: Map<string, CircuitBreaker> = new Map();\n  private healingInProgress: Set<string> = new Set();\n\n  constructor(\n    eventPublisher: EventPublisher,\n    eventSubscriber: EventSubscriber,\n    metricsCollector: MetricsCollector,\n    tracer: OpenTelemetryTracer,\n    logger: Logger\n  ) {\n    this.eventPublisher = eventPublisher;\n    this.eventSubscriber = eventSubscriber;\n    this.metricsCollector = metricsCollector;\n    this.tracer = tracer;\n    this.logger = logger;\n    \n    this.initializeDefaultRules();\n    this.setupEventSubscriptions();\n    this.startMonitoring();\n  }\n\n  private initializeDefaultRules(): void {\n    const defaultRules: IncidentDetectionRule[] = [\n      {\n        id: 'high_response_time',\n        name: 'High Response Time',\n        description: 'API response time exceeds 800ms threshold',\n        metricName: 'http_request_duration_ms',\n        threshold: 800,\n        operator: 'gt',\n        windowMinutes: 5,\n        severity: 'high',\n        enabled: true,\n        cooldownMinutes: 10,\n        autoHealingActions: [\n          {\n            id: 'clear_cache',\n            type: 'clear_cache',\n            parameters: { cacheType: 'redis', pattern: '*' },\n            maxRetries: 2,\n            retryDelaySeconds: 30,\n            successCriteria: {\n              metricName: 'http_request_duration_ms',\n              expectedValue: 600,\n              operator: 'lt',\n              timeoutMinutes: 5\n            }\n          },\n          {\n            id: 'scale_up',\n            type: 'scale_up',\n            parameters: { service: 'api', scaleFactor: 1.5 },\n            maxRetries: 1,\n            retryDelaySeconds: 60,\n            successCriteria: {\n              metricName: 'http_request_duration_ms',\n              expectedValue: 600,\n              operator: 'lt',\n              timeoutMinutes: 10\n            }\n          }\n        ]\n      },\n      {\n        id: 'high_error_rate',\n        name: 'High Error Rate',\n        description: 'Error rate exceeds 5% threshold',\n        metricName: 'error_rate_percent',\n        threshold: 5,\n        operator: 'gt',\n        windowMinutes: 3,\n        severity: 'critical',\n        enabled: true,\n        cooldownMinutes: 15,\n        autoHealingActions: [\n          {\n            id: 'circuit_breaker',\n            type: 'circuit_breaker',\n            parameters: { service: 'failing_service', action: 'open' },\n            maxRetries: 1,\n            retryDelaySeconds: 0,\n            successCriteria: {\n              metricName: 'error_rate_percent',\n              expectedValue: 2,\n              operator: 'lt',\n              timeoutMinutes: 5\n            }\n          },\n          {\n            id: 'restart_service',\n            type: 'restart_service',\n            parameters: { service: 'failing_service' },\n            maxRetries: 2,\n            retryDelaySeconds: 120,\n            successCriteria: {\n              metricName: 'error_rate_percent',\n              expectedValue: 2,\n              operator: 'lt',\n              timeoutMinutes: 10\n            }\n          }\n        ]\n      },\n      {\n        id: 'memory_usage_high',\n        name: 'High Memory Usage',\n        description: 'Memory usage exceeds 85% threshold',\n        metricName: 'memory_usage_percent',\n        threshold: 85,\n        operator: 'gt',\n        windowMinutes: 5,\n        severity: 'medium',\n        enabled: true,\n        cooldownMinutes: 20,\n        autoHealingActions: [\n          {\n            id: 'clear_cache',\n            type: 'clear_cache',\n            parameters: { cacheType: 'memory', aggressive: true },\n            maxRetries: 1,\n            retryDelaySeconds: 30,\n            successCriteria: {\n              metricName: 'memory_usage_percent',\n              expectedValue: 70,\n              operator: 'lt',\n              timeoutMinutes: 5\n            }\n          },\n          {\n            id: 'restart_service',\n            type: 'restart_service',\n            parameters: { service: 'memory_intensive_service' },\n            maxRetries: 1,\n            retryDelaySeconds: 180,\n            successCriteria: {\n              metricName: 'memory_usage_percent',\n              expectedValue: 50,\n              operator: 'lt',\n              timeoutMinutes: 10\n            }\n          }\n        ]\n      },\n      {\n        id: 'cold_start_threshold',\n        name: 'Cold Start Performance',\n        description: 'Lambda cold starts exceed 150ms threshold',\n        metricName: 'lambda_cold_start_duration_ms',\n        threshold: 150,\n        operator: 'gt',\n        windowMinutes: 10,\n        severity: 'medium',\n        enabled: true,\n        cooldownMinutes: 30,\n        autoHealingActions: [\n          {\n            id: 'warm_lambdas',\n            type: 'custom',\n            parameters: { \n              action: 'warm_lambda_functions',\n              functions: ['storytailor-router', 'storytailor-content-agent']\n            },\n            maxRetries: 1,\n            retryDelaySeconds: 60,\n            successCriteria: {\n              metricName: 'lambda_cold_start_duration_ms',\n              expectedValue: 120,\n              operator: 'lt',\n              timeoutMinutes: 15\n            }\n          }\n        ]\n      }\n    ];\n\n    defaultRules.forEach(rule => {\n      this.detectionRules.set(rule.id, rule);\n    });\n\n    this.logger.info(`Initialized ${defaultRules.length} default incident detection rules`);\n  }\n\n  private setupEventSubscriptions(): void {\n    // Subscribe to metric events\n    this.eventSubscriber.subscribe('metric.collected', async (event) => {\n      await this.processMetricEvent(event.data);\n    });\n\n    // Subscribe to error events\n    this.eventSubscriber.subscribe('error.occurred', async (event) => {\n      await this.processErrorEvent(event.data);\n    });\n\n    // Subscribe to performance events\n    this.eventSubscriber.subscribe('performance.degraded', async (event) => {\n      await this.processPerformanceEvent(event.data);\n    });\n\n    this.logger.info('Self-healing monitor event subscriptions established');\n  }\n\n  private startMonitoring(): void {\n    // Start periodic metric evaluation\n    setInterval(async () => {\n      await this.evaluateDetectionRules();\n    }, 30000); // Every 30 seconds\n\n    // Start incident status monitoring\n    setInterval(async () => {\n      await this.monitorActiveIncidents();\n    }, 60000); // Every minute\n\n    // Start knowledge base maintenance\n    setInterval(async () => {\n      await this.maintainKnowledgeBase();\n    }, 3600000); // Every hour\n\n    this.logger.info('Self-healing monitoring started');\n  }\n\n  private async processMetricEvent(metricData: any): Promise<void> {\n    const { name, value, timestamp, tags } = metricData;\n    \n    // Store metric history\n    if (!this.metricHistory.has(name)) {\n      this.metricHistory.set(name, []);\n    }\n    \n    const history = this.metricHistory.get(name)!;\n    history.push({ timestamp: new Date(timestamp), value });\n    \n    // Keep only recent history (last 24 hours)\n    const cutoff = new Date(Date.now() - 24 * 60 * 60 * 1000);\n    this.metricHistory.set(name, history.filter(h => h.timestamp > cutoff));\n    \n    // Check if this metric triggers any detection rules\n    await this.checkMetricAgainstRules(name, value, tags);\n  }\n\n  private async processErrorEvent(errorData: any): Promise<void> {\n    const { error, service, severity, context } = errorData;\n    \n    // Create or update error rate metric\n    const errorRateMetric = {\n      name: 'error_rate_percent',\n      value: await this.calculateErrorRate(service),\n      timestamp: new Date(),\n      tags: { service }\n    };\n    \n    await this.processMetricEvent(errorRateMetric);\n    \n    // Check for error patterns in knowledge base\n    await this.checkErrorPatterns(error, service, context);\n  }\n\n  private async processPerformanceEvent(performanceData: any): Promise<void> {\n    const { metric, value, service, threshold } = performanceData;\n    \n    if (value > threshold) {\n      // Create performance degradation incident\n      await this.createIncident({\n        ruleId: 'performance_degradation',\n        ruleName: 'Performance Degradation',\n        severity: 'medium',\n        description: `Performance metric ${metric} exceeded threshold`,\n        metricValue: value,\n        threshold,\n        context: { service, metric }\n      });\n    }\n  }\n\n  private async evaluateDetectionRules(): Promise<void> {\n    for (const [ruleId, rule] of this.detectionRules) {\n      if (!rule.enabled) continue;\n      \n      // Check cooldown\n      const lastTriggered = this.cooldownTracker.get(ruleId);\n      if (lastTriggered) {\n        const cooldownEnd = new Date(lastTriggered.getTime() + rule.cooldownMinutes * 60 * 1000);\n        if (new Date() < cooldownEnd) {\n          continue;\n        }\n      }\n      \n      await this.evaluateRule(rule);\n    }\n  }\n\n  private async evaluateRule(rule: IncidentDetectionRule): Promise<void> {\n    const span = this.tracer.startSpan('evaluate_detection_rule', {\n      'rule.id': rule.id,\n      'rule.name': rule.name\n    });\n    \n    try {\n      const metricHistory = this.metricHistory.get(rule.metricName);\n      if (!metricHistory || metricHistory.length === 0) {\n        return;\n      }\n      \n      // Get metrics within the time window\n      const windowStart = new Date(Date.now() - rule.windowMinutes * 60 * 1000);\n      const windowMetrics = metricHistory.filter(m => m.timestamp >= windowStart);\n      \n      if (windowMetrics.length === 0) {\n        return;\n      }\n      \n      // Calculate aggregate value (average for now)\n      const avgValue = windowMetrics.reduce((sum, m) => sum + m.value, 0) / windowMetrics.length;\n      \n      // Check if threshold is breached\n      const isBreached = this.evaluateThreshold(avgValue, rule.threshold, rule.operator);\n      \n      if (isBreached) {\n        await this.createIncident({\n          ruleId: rule.id,\n          ruleName: rule.name,\n          severity: rule.severity,\n          description: rule.description,\n          metricValue: avgValue,\n          threshold: rule.threshold,\n          context: {\n            windowMinutes: rule.windowMinutes,\n            sampleCount: windowMetrics.length,\n            operator: rule.operator\n          }\n        });\n        \n        this.cooldownTracker.set(rule.id, new Date());\n      }\n    } catch (error) {\n      this.logger.error('Error evaluating detection rule', {\n        ruleId: rule.id,\n        error: error.message\n      });\n      span.recordException(error as Error);\n    } finally {\n      span.end();\n    }\n  }\n\n  private evaluateThreshold(value: number, threshold: number, operator: string): boolean {\n    switch (operator) {\n      case 'gt': return value > threshold;\n      case 'gte': return value >= threshold;\n      case 'lt': return value < threshold;\n      case 'lte': return value <= threshold;\n      case 'eq': return value === threshold;\n      default: return false;\n    }\n  }\n\n  private async createIncident(incidentData: {\n    ruleId: string;\n    ruleName: string;\n    severity: string;\n    description: string;\n    metricValue: number;\n    threshold: number;\n    context: Record<string, any>;\n  }): Promise<void> {\n    const incidentId = `incident_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    const incident: Incident = {\n      id: incidentId,\n      ruleId: incidentData.ruleId,\n      ruleName: incidentData.ruleName,\n      severity: incidentData.severity,\n      description: incidentData.description,\n      detectedAt: new Date(),\n      status: 'open',\n      metricValue: incidentData.metricValue,\n      threshold: incidentData.threshold,\n      healingActions: [],\n      context: incidentData.context\n    };\n    \n    this.activeIncidents.set(incidentId, incident);\n    \n    // Publish incident event\n    await this.eventPublisher.publish({\n      type: 'incident.detected',\n      data: incident,\n      timestamp: new Date(),\n      source: 'self-healing-monitor'\n    });\n    \n    this.logger.warn('Incident detected', {\n      incidentId,\n      ruleId: incidentData.ruleId,\n      severity: incidentData.severity,\n      metricValue: incidentData.metricValue,\n      threshold: incidentData.threshold\n    });\n    \n    // Start auto-healing if configured\n    const rule = this.detectionRules.get(incidentData.ruleId);\n    if (rule && rule.autoHealingActions.length > 0) {\n      await this.startAutoHealing(incident, rule.autoHealingActions);\n    }\n  }\n\n  private async startAutoHealing(incident: Incident, actions: AutoHealingAction[]): Promise<void> {\n    if (this.healingInProgress.has(incident.id)) {\n      this.logger.warn('Auto-healing already in progress for incident', { incidentId: incident.id });\n      return;\n    }\n    \n    this.healingInProgress.add(incident.id);\n    incident.status = 'healing';\n    \n    this.logger.info('Starting auto-healing', {\n      incidentId: incident.id,\n      actionsCount: actions.length\n    });\n    \n    try {\n      for (const action of actions) {\n        const actionResult: HealingActionResult = {\n          actionId: action.id,\n          actionType: action.type,\n          startedAt: new Date(),\n          status: 'pending',\n          attempts: 0,\n          maxRetries: action.maxRetries,\n          logs: []\n        };\n        \n        incident.healingActions.push(actionResult);\n        \n        const success = await this.executeHealingAction(action, actionResult);\n        \n        if (success) {\n          // Check if incident is resolved\n          const isResolved = await this.checkIncidentResolution(incident, action.successCriteria);\n          if (isResolved) {\n            incident.status = 'resolved';\n            incident.resolvedAt = new Date();\n            \n            await this.eventPublisher.publish({\n              type: 'incident.resolved',\n              data: incident,\n              timestamp: new Date(),\n              source: 'self-healing-monitor'\n            });\n            \n            this.logger.info('Incident resolved through auto-healing', {\n              incidentId: incident.id,\n              resolvedBy: action.id\n            });\n            \n            // Update knowledge base\n            await this.updateKnowledgeBase(incident, action, true);\n            break;\n          }\n        } else {\n          this.logger.warn('Auto-healing action failed', {\n            incidentId: incident.id,\n            actionId: action.id,\n            attempts: actionResult.attempts\n          });\n        }\n      }\n      \n      // If no action resolved the incident\n      if (incident.status === 'healing') {\n        incident.status = 'failed';\n        \n        await this.eventPublisher.publish({\n          type: 'incident.healing_failed',\n          data: incident,\n          timestamp: new Date(),\n          source: 'self-healing-monitor'\n        });\n        \n        this.logger.error('Auto-healing failed for incident', {\n          incidentId: incident.id,\n          actionsAttempted: actions.length\n        });\n      }\n    } finally {\n      this.healingInProgress.delete(incident.id);\n    }\n  }\n\n  private async executeHealingAction(action: AutoHealingAction, result: HealingActionResult): Promise<boolean> {\n    const span = this.tracer.startSpan('execute_healing_action', {\n      'action.id': action.id,\n      'action.type': action.type\n    });\n    \n    try {\n      result.status = 'running';\n      result.attempts++;\n      result.logs.push(`Starting ${action.type} action (attempt ${result.attempts})`);\n      \n      let success = false;\n      \n      switch (action.type) {\n        case 'restart_service':\n          success = await this.restartService(action.parameters, result);\n          break;\n        case 'scale_up':\n          success = await this.scaleUpService(action.parameters, result);\n          break;\n        case 'circuit_breaker':\n          success = await this.toggleCircuitBreaker(action.parameters, result);\n          break;\n        case 'rollback':\n          success = await this.rollbackDeployment(action.parameters, result);\n          break;\n        case 'clear_cache':\n          success = await this.clearCache(action.parameters, result);\n          break;\n        case 'custom':\n          success = await this.executeCustomAction(action.parameters, result);\n          break;\n        default:\n          result.logs.push(`Unknown action type: ${action.type}`);\n          success = false;\n      }\n      \n      if (success) {\n        result.status = 'success';\n        result.completedAt = new Date();\n        result.logs.push('Action completed successfully');\n      } else if (result.attempts < action.maxRetries) {\n        // Retry after delay\n        result.logs.push(`Action failed, retrying in ${action.retryDelaySeconds} seconds`);\n        await new Promise(resolve => setTimeout(resolve, action.retryDelaySeconds * 1000));\n        return await this.executeHealingAction(action, result);\n      } else {\n        result.status = 'failed';\n        result.completedAt = new Date();\n        result.logs.push('Action failed after all retries');\n      }\n      \n      return success;\n    } catch (error) {\n      result.status = 'failed';\n      result.completedAt = new Date();\n      result.error = error.message;\n      result.logs.push(`Action failed with error: ${error.message}`);\n      \n      this.logger.error('Healing action execution error', {\n        actionId: action.id,\n        error: error.message\n      });\n      \n      span.recordException(error as Error);\n      return false;\n    } finally {\n      span.end();\n    }\n  }\n\n  private async restartService(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { service } = parameters;\n    result.logs.push(`Restarting service: ${service}`);\n    \n    // Implementation would depend on your deployment platform\n    // This is a placeholder for the actual restart logic\n    try {\n      // Example: Kubernetes restart\n      // await kubectl.restartDeployment(service);\n      \n      // Example: Docker restart\n      // await docker.restartContainer(service);\n      \n      // Example: Lambda function restart (update configuration)\n      // await lambda.updateFunctionConfiguration({ FunctionName: service });\n      \n      result.logs.push(`Service ${service} restart initiated`);\n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to restart service ${service}: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async scaleUpService(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { service, scaleFactor } = parameters;\n    result.logs.push(`Scaling up service: ${service} by factor ${scaleFactor}`);\n    \n    try {\n      // Implementation would depend on your deployment platform\n      // Example: Kubernetes horizontal pod autoscaler\n      // await kubectl.scaleDeployment(service, Math.ceil(currentReplicas * scaleFactor));\n      \n      result.logs.push(`Service ${service} scaled up successfully`);\n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to scale up service ${service}: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async toggleCircuitBreaker(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { service, action } = parameters;\n    result.logs.push(`${action === 'open' ? 'Opening' : 'Closing'} circuit breaker for service: ${service}`);\n    \n    try {\n      let circuitBreaker = this.circuitBreakers.get(service);\n      if (!circuitBreaker) {\n        circuitBreaker = new CircuitBreaker({\n          failureThreshold: 5,\n          recoveryTimeout: 60000,\n          monitoringPeriod: 10000\n        });\n        this.circuitBreakers.set(service, circuitBreaker);\n      }\n      \n      if (action === 'open') {\n        circuitBreaker.forceOpen();\n      } else {\n        circuitBreaker.forceClose();\n      }\n      \n      result.logs.push(`Circuit breaker ${action}ed for service ${service}`);\n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to ${action} circuit breaker for ${service}: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async rollbackDeployment(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { service, version } = parameters;\n    result.logs.push(`Rolling back service: ${service} to version ${version || 'previous'}`);\n    \n    try {\n      // Implementation would depend on your deployment platform\n      // Example: Kubernetes rollback\n      // await kubectl.rollbackDeployment(service, version);\n      \n      result.logs.push(`Service ${service} rolled back successfully`);\n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to rollback service ${service}: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async clearCache(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { cacheType, pattern, aggressive } = parameters;\n    result.logs.push(`Clearing ${cacheType} cache with pattern: ${pattern || '*'}`);\n    \n    try {\n      // Implementation would depend on your caching system\n      if (cacheType === 'redis') {\n        // Example: Redis cache clear\n        // await redis.flushdb();\n        // or await redis.del(await redis.keys(pattern));\n      } else if (cacheType === 'memory') {\n        // Example: In-memory cache clear\n        // await memoryCache.clear();\n      }\n      \n      result.logs.push(`${cacheType} cache cleared successfully`);\n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to clear ${cacheType} cache: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async executeCustomAction(parameters: any, result: HealingActionResult): Promise<boolean> {\n    const { action } = parameters;\n    result.logs.push(`Executing custom action: ${action}`);\n    \n    try {\n      switch (action) {\n        case 'warm_lambda_functions':\n          return await this.warmLambdaFunctions(parameters.functions, result);\n        default:\n          result.logs.push(`Unknown custom action: ${action}`);\n          return false;\n      }\n    } catch (error) {\n      result.logs.push(`Custom action failed: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async warmLambdaFunctions(functions: string[], result: HealingActionResult): Promise<boolean> {\n    result.logs.push(`Warming ${functions.length} Lambda functions`);\n    \n    try {\n      // Implementation to warm Lambda functions\n      // This would typically involve invoking each function with a warming payload\n      for (const functionName of functions) {\n        // Example: AWS Lambda invoke\n        // await lambda.invoke({\n        //   FunctionName: functionName,\n        //   InvocationType: 'Event',\n        //   Payload: JSON.stringify({ warmup: true })\n        // }).promise();\n        \n        result.logs.push(`Warmed function: ${functionName}`);\n      }\n      \n      return true;\n    } catch (error) {\n      result.logs.push(`Failed to warm Lambda functions: ${error.message}`);\n      return false;\n    }\n  }\n\n  private async checkIncidentResolution(incident: Incident, criteria: SuccessCriteria): Promise<boolean> {\n    const timeoutEnd = new Date(Date.now() + criteria.timeoutMinutes * 60 * 1000);\n    \n    while (new Date() < timeoutEnd) {\n      const metricHistory = this.metricHistory.get(criteria.metricName);\n      if (metricHistory && metricHistory.length > 0) {\n        const latestValue = metricHistory[metricHistory.length - 1].value;\n        \n        if (this.evaluateThreshold(latestValue, criteria.expectedValue, criteria.operator)) {\n          return true;\n        }\n      }\n      \n      // Wait before checking again\n      await new Promise(resolve => setTimeout(resolve, 30000)); // 30 seconds\n    }\n    \n    return false;\n  }\n\n  private async monitorActiveIncidents(): Promise<void> {\n    for (const [incidentId, incident] of this.activeIncidents) {\n      // Remove resolved incidents after 24 hours\n      if (incident.status === 'resolved' && incident.resolvedAt) {\n        const dayAgo = new Date(Date.now() - 24 * 60 * 60 * 1000);\n        if (incident.resolvedAt < dayAgo) {\n          this.activeIncidents.delete(incidentId);\n          continue;\n        }\n      }\n      \n      // Check for stuck healing processes\n      if (incident.status === 'healing') {\n        const healingStarted = incident.healingActions[0]?.startedAt;\n        if (healingStarted) {\n          const hourAgo = new Date(Date.now() - 60 * 60 * 1000);\n          if (healingStarted < hourAgo) {\n            this.logger.warn('Healing process appears stuck', {\n              incidentId,\n              healingStarted\n            });\n            \n            // Force incident to failed status\n            incident.status = 'failed';\n            this.healingInProgress.delete(incidentId);\n          }\n        }\n      }\n    }\n  }\n\n  private async calculateErrorRate(service: string): Promise<number> {\n    // Implementation to calculate error rate for a service\n    // This would typically query your metrics system\n    return 0; // Placeholder\n  }\n\n  private async checkErrorPatterns(error: string, service: string, context: any): Promise<void> {\n    // Check if this error matches any known patterns in the knowledge base\n    for (const [id, entry] of this.knowledgeBase) {\n      if (entry.symptoms.some(symptom => error.includes(symptom))) {\n        this.logger.info('Known error pattern detected', {\n          patternId: id,\n          service,\n          solution: entry.solution\n        });\n        \n        // Could trigger automatic remediation based on knowledge base\n        break;\n      }\n    }\n  }\n\n  private async updateKnowledgeBase(incident: Incident, successfulAction: AutoHealingAction, success: boolean): Promise<void> {\n    const entryId = `${incident.ruleId}_${successfulAction.id}`;\n    \n    let entry = this.knowledgeBase.get(entryId);\n    if (!entry) {\n      entry = {\n        id: entryId,\n        incidentPattern: incident.ruleName,\n        symptoms: [incident.description],\n        rootCause: `Metric ${incident.ruleId} exceeded threshold`,\n        solution: `Applied ${successfulAction.type} action`,\n        preventionSteps: [],\n        successRate: success ? 100 : 0,\n        lastUpdated: new Date(),\n        tags: [incident.severity, successfulAction.type]\n      };\n    } else {\n      // Update success rate\n      const totalAttempts = entry.successRate === 100 ? 1 : Math.round(100 / entry.successRate);\n      const newSuccessRate = success \n        ? ((entry.successRate * totalAttempts) + 100) / (totalAttempts + 1)\n        : (entry.successRate * totalAttempts) / (totalAttempts + 1);\n      \n      entry.successRate = newSuccessRate;\n      entry.lastUpdated = new Date();\n    }\n    \n    this.knowledgeBase.set(entryId, entry);\n    \n    this.logger.info('Knowledge base updated', {\n      entryId,\n      successRate: entry.successRate,\n      success\n    });\n  }\n\n  private async maintainKnowledgeBase(): Promise<void> {\n    // Remove old entries with low success rates\n    const cutoffDate = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000); // 30 days\n    \n    for (const [id, entry] of this.knowledgeBase) {\n      if (entry.lastUpdated < cutoffDate && entry.successRate < 20) {\n        this.knowledgeBase.delete(id);\n        this.logger.info('Removed low-success knowledge base entry', {\n          entryId: id,\n          successRate: entry.successRate\n        });\n      }\n    }\n  }\n\n  // Public API methods\n  public addDetectionRule(rule: IncidentDetectionRule): void {\n    this.detectionRules.set(rule.id, rule);\n    this.logger.info('Detection rule added', { ruleId: rule.id, ruleName: rule.name });\n  }\n\n  public removeDetectionRule(ruleId: string): void {\n    this.detectionRules.delete(ruleId);\n    this.logger.info('Detection rule removed', { ruleId });\n  }\n\n  public getActiveIncidents(): Incident[] {\n    return Array.from(this.activeIncidents.values());\n  }\n\n  public getIncident(incidentId: string): Incident | undefined {\n    return this.activeIncidents.get(incidentId);\n  }\n\n  public getKnowledgeBase(): KnowledgeBaseEntry[] {\n    return Array.from(this.knowledgeBase.values());\n  }\n\n  public async manuallyResolveIncident(incidentId: string, resolution: string): Promise<void> {\n    const incident = this.activeIncidents.get(incidentId);\n    if (!incident) {\n      throw new Error(`Incident ${incidentId} not found`);\n    }\n    \n    incident.status = 'resolved';\n    incident.resolvedAt = new Date();\n    \n    await this.eventPublisher.publish({\n      type: 'incident.manually_resolved',\n      data: { incident, resolution },\n      timestamp: new Date(),\n      source: 'self-healing-monitor'\n    });\n    \n    this.logger.info('Incident manually resolved', {\n      incidentId,\n      resolution\n    });\n  }\n\n  public getMetrics(): any {\n    return {\n      activeIncidents: this.activeIncidents.size,\n      detectionRules: this.detectionRules.size,\n      knowledgeBaseEntries: this.knowledgeBase.size,\n      healingInProgress: this.healingInProgress.size,\n      metricHistorySize: Array.from(this.metricHistory.values()).reduce((sum, history) => sum + history.length, 0)\n    };\n  }\n}"