// Enhanced Alerting System with Self-Healing Integration\nimport { EventPublisher } from '../EventPublisher';\nimport { EventSubscriber } from '../EventSubscriber';\nimport { Logger } from 'winston';\nimport { SelfHealingMonitor, Incident } from './SelfHealingMonitor';\n\nexport interface AlertRule {\n  id: string;\n  name: string;\n  description: string;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  conditions: AlertCondition[];\n  channels: AlertChannel[];\n  enabled: boolean;\n  cooldownMinutes: number;\n  escalationRules?: EscalationRule[];\n  suppressionRules?: SuppressionRule[];\n}\n\nexport interface AlertCondition {\n  type: 'metric' | 'event' | 'incident' | 'log';\n  source: string;\n  operator: 'gt' | 'lt' | 'eq' | 'gte' | 'lte' | 'contains' | 'not_contains';\n  value: any;\n  windowMinutes?: number;\n  aggregation?: 'avg' | 'sum' | 'count' | 'max' | 'min';\n}\n\nexport interface AlertChannel {\n  type: 'email' | 'slack' | 'pagerduty' | 'webhook' | 'sms';\n  config: Record<string, any>;\n  enabled: boolean;\n}\n\nexport interface EscalationRule {\n  delayMinutes: number;\n  channels: AlertChannel[];\n  condition?: 'unresolved' | 'worsening';\n}\n\nexport interface SuppressionRule {\n  type: 'time_window' | 'dependency' | 'maintenance';\n  config: Record<string, any>;\n}\n\nexport interface Alert {\n  id: string;\n  ruleId: string;\n  ruleName: string;\n  severity: string;\n  title: string;\n  description: string;\n  triggeredAt: Date;\n  resolvedAt?: Date;\n  status: 'open' | 'acknowledged' | 'resolved' | 'suppressed';\n  context: Record<string, any>;\n  notifications: AlertNotification[];\n  escalations: AlertEscalation[];\n  incidentId?: string;\n}\n\nexport interface AlertNotification {\n  id: string;\n  channel: AlertChannel;\n  sentAt: Date;\n  status: 'pending' | 'sent' | 'failed' | 'delivered';\n  error?: string;\n  response?: any;\n}\n\nexport interface AlertEscalation {\n  level: number;\n  triggeredAt: Date;\n  channels: AlertChannel[];\n  notifications: AlertNotification[];\n}\n\nexport class AlertingSystem {\n  private eventPublisher: EventPublisher;\n  private eventSubscriber: EventSubscriber;\n  private logger: Logger;\n  private selfHealingMonitor?: SelfHealingMonitor;\n  private alertRules: Map<string, AlertRule> = new Map();\n  private activeAlerts: Map<string, Alert> = new Map();\n  private cooldownTracker: Map<string, Date> = new Map();\n  private suppressionStates: Map<string, boolean> = new Map();\n  private escalationTimers: Map<string, NodeJS.Timeout> = new Map();\n\n  constructor(\n    eventPublisher: EventPublisher,\n    eventSubscriber: EventSubscriber,\n    logger: Logger,\n    selfHealingMonitor?: SelfHealingMonitor\n  ) {\n    this.eventPublisher = eventPublisher;\n    this.eventSubscriber = eventSubscriber;\n    this.logger = logger;\n    this.selfHealingMonitor = selfHealingMonitor;\n    \n    this.initializeDefaultRules();\n    this.setupEventSubscriptions();\n    this.startAlertMonitoring();\n  }\n\n  private initializeDefaultRules(): void {\n    const defaultRules: AlertRule[] = [\n      {\n        id: 'critical_incident_detected',\n        name: 'Critical Incident Detected',\n        description: 'A critical incident has been detected by the self-healing system',\n        severity: 'critical',\n        conditions: [\n          {\n            type: 'incident',\n            source: 'self-healing-monitor',\n            operator: 'eq',\n            value: 'critical'\n          }\n        ],\n        channels: [\n          {\n            type: 'pagerduty',\n            config: { serviceKey: process.env.PAGERDUTY_SERVICE_KEY },\n            enabled: true\n          },\n          {\n            type: 'slack',\n            config: { \n              webhook: process.env.SLACK_CRITICAL_WEBHOOK,\n              channel: '#critical-alerts'\n            },\n            enabled: true\n          }\n        ],\n        enabled: true,\n        cooldownMinutes: 5,\n        escalationRules: [\n          {\n            delayMinutes: 15,\n            condition: 'unresolved',\n            channels: [\n              {\n                type: 'email',\n                config: { \n                  recipients: ['oncall@storytailor.com', 'cto@storytailor.com'],\n                  subject: 'ESCALATED: Critical Incident Unresolved'\n                },\n                enabled: true\n              }\n            ]\n          }\n        ]\n      },\n      {\n        id: 'auto_healing_failed',\n        name: 'Auto-Healing Failed',\n        description: 'Automated healing attempts have failed for an incident',\n        severity: 'high',\n        conditions: [\n          {\n            type: 'event',\n            source: 'self-healing-monitor',\n            operator: 'eq',\n            value: 'incident.healing_failed'\n          }\n        ],\n        channels: [\n          {\n            type: 'slack',\n            config: { \n              webhook: process.env.SLACK_ALERTS_WEBHOOK,\n              channel: '#alerts'\n            },\n            enabled: true\n          },\n          {\n            type: 'email',\n            config: { \n              recipients: ['devops@storytailor.com'],\n              subject: 'Auto-Healing Failed - Manual Intervention Required'\n            },\n            enabled: true\n          }\n        ],\n        enabled: true,\n        cooldownMinutes: 10\n      },\n      {\n        id: 'high_error_rate',\n        name: 'High Error Rate',\n        description: 'System error rate has exceeded acceptable thresholds',\n        severity: 'high',\n        conditions: [\n          {\n            type: 'metric',\n            source: 'error_rate_percent',\n            operator: 'gt',\n            value: 5,\n            windowMinutes: 5,\n            aggregation: 'avg'\n          }\n        ],\n        channels: [\n          {\n            type: 'slack',\n            config: { \n              webhook: process.env.SLACK_ALERTS_WEBHOOK,\n              channel: '#alerts'\n            },\n            enabled: true\n          }\n        ],\n        enabled: true,\n        cooldownMinutes: 15\n      },\n      {\n        id: 'performance_degradation',\n        name: 'Performance Degradation',\n        description: 'System performance has degraded significantly',\n        severity: 'medium',\n        conditions: [\n          {\n            type: 'metric',\n            source: 'http_request_duration_ms',\n            operator: 'gt',\n            value: 1000,\n            windowMinutes: 10,\n            aggregation: 'avg'\n          }\n        ],\n        channels: [\n          {\n            type: 'slack',\n            config: { \n              webhook: process.env.SLACK_ALERTS_WEBHOOK,\n              channel: '#performance'\n            },\n            enabled: true\n          }\n        ],\n        enabled: true,\n        cooldownMinutes: 20\n      },\n      {\n        id: 'compliance_violation',\n        name: 'Compliance Violation Detected',\n        description: 'A potential compliance violation has been detected',\n        severity: 'critical',\n        conditions: [\n          {\n            type: 'event',\n            source: 'compliance-monitor',\n            operator: 'contains',\n            value: 'violation'\n          }\n        ],\n        channels: [\n          {\n            type: 'email',\n            config: { \n              recipients: ['legal@storytailor.com', 'privacy@storytailor.com'],\n              subject: 'URGENT: Compliance Violation Detected'\n            },\n            enabled: true\n          },\n          {\n            type: 'pagerduty',\n            config: { serviceKey: process.env.PAGERDUTY_COMPLIANCE_KEY },\n            enabled: true\n          }\n        ],\n        enabled: true,\n        cooldownMinutes: 0 // No cooldown for compliance issues\n      }\n    ];\n\n    defaultRules.forEach(rule => {\n      this.alertRules.set(rule.id, rule);\n    });\n\n    this.logger.info(`Initialized ${defaultRules.length} default alert rules`);\n  }\n\n  private setupEventSubscriptions(): void {\n    // Subscribe to incident events from self-healing monitor\n    this.eventSubscriber.subscribe('incident.detected', async (event) => {\n      await this.processIncidentEvent(event.data);\n    });\n\n    this.eventSubscriber.subscribe('incident.resolved', async (event) => {\n      await this.processIncidentResolution(event.data);\n    });\n\n    this.eventSubscriber.subscribe('incident.healing_failed', async (event) => {\n      await this.processHealingFailure(event.data);\n    });\n\n    // Subscribe to metric events\n    this.eventSubscriber.subscribe('metric.collected', async (event) => {\n      await this.processMetricEvent(event.data);\n    });\n\n    // Subscribe to error events\n    this.eventSubscriber.subscribe('error.occurred', async (event) => {\n      await this.processErrorEvent(event.data);\n    });\n\n    // Subscribe to compliance events\n    this.eventSubscriber.subscribe('compliance.violation', async (event) => {\n      await this.processComplianceEvent(event.data);\n    });\n\n    this.logger.info('Alerting system event subscriptions established');\n  }\n\n  private startAlertMonitoring(): void {\n    // Monitor alert escalations\n    setInterval(async () => {\n      await this.processEscalations();\n    }, 60000); // Every minute\n\n    // Clean up resolved alerts\n    setInterval(async () => {\n      await this.cleanupResolvedAlerts();\n    }, 300000); // Every 5 minutes\n\n    this.logger.info('Alert monitoring started');\n  }\n\n  private async processIncidentEvent(incident: Incident): Promise<void> {\n    // Check if this incident should trigger alerts\n    for (const [ruleId, rule] of this.alertRules) {\n      if (!rule.enabled) continue;\n      \n      const shouldAlert = rule.conditions.some(condition => {\n        if (condition.type === 'incident') {\n          return condition.operator === 'eq' && incident.severity === condition.value;\n        }\n        return false;\n      });\n      \n      if (shouldAlert) {\n        await this.createAlert(rule, {\n          title: `${rule.name}: ${incident.ruleName}`,\n          description: `Incident detected: ${incident.description}`,\n          context: {\n            incidentId: incident.id,\n            severity: incident.severity,\n            metricValue: incident.metricValue,\n            threshold: incident.threshold\n          },\n          incidentId: incident.id\n        });\n      }\n    }\n  }\n\n  private async processIncidentResolution(incident: Incident): Promise<void> {\n    // Find and resolve related alerts\n    for (const [alertId, alert] of this.activeAlerts) {\n      if (alert.incidentId === incident.id && alert.status === 'open') {\n        await this.resolveAlert(alertId, 'Incident automatically resolved by self-healing system');\n      }\n    }\n  }\n\n  private async processHealingFailure(incident: Incident): Promise<void> {\n    // Trigger specific alert for healing failures\n    const rule = this.alertRules.get('auto_healing_failed');\n    if (rule && rule.enabled) {\n      await this.createAlert(rule, {\n        title: `Auto-Healing Failed: ${incident.ruleName}`,\n        description: `Automated healing attempts failed for incident: ${incident.description}`,\n        context: {\n          incidentId: incident.id,\n          severity: incident.severity,\n          healingActions: incident.healingActions.length,\n          failedActions: incident.healingActions.filter(a => a.status === 'failed').length\n        },\n        incidentId: incident.id\n      });\n    }\n  }\n\n  private async processMetricEvent(metricData: any): Promise<void> {\n    const { name, value, timestamp, tags } = metricData;\n    \n    // Check metric-based alert rules\n    for (const [ruleId, rule] of this.alertRules) {\n      if (!rule.enabled) continue;\n      \n      const metricConditions = rule.conditions.filter(c => c.type === 'metric' && c.source === name);\n      \n      for (const condition of metricConditions) {\n        const shouldAlert = this.evaluateMetricCondition(condition, value);\n        \n        if (shouldAlert) {\n          await this.createAlert(rule, {\n            title: `${rule.name}: ${name}`,\n            description: `Metric ${name} ${condition.operator} ${condition.value} (current: ${value})`,\n            context: {\n              metricName: name,\n              metricValue: value,\n              threshold: condition.value,\n              operator: condition.operator,\n              tags\n            }\n          });\n        }\n      }\n    }\n  }\n\n  private async processErrorEvent(errorData: any): Promise<void> {\n    // Process error events for alerting\n    const { error, service, severity, context } = errorData;\n    \n    // This could trigger error rate calculations or specific error pattern alerts\n    // Implementation depends on your error tracking requirements\n  }\n\n  private async processComplianceEvent(complianceData: any): Promise<void> {\n    const rule = this.alertRules.get('compliance_violation');\n    if (rule && rule.enabled) {\n      await this.createAlert(rule, {\n        title: `Compliance Violation: ${complianceData.type}`,\n        description: `Compliance violation detected: ${complianceData.description}`,\n        context: {\n          violationType: complianceData.type,\n          regulation: complianceData.regulation,\n          severity: complianceData.severity,\n          details: complianceData.details\n        }\n      });\n    }\n  }\n\n  private evaluateMetricCondition(condition: AlertCondition, value: number): boolean {\n    switch (condition.operator) {\n      case 'gt': return value > condition.value;\n      case 'gte': return value >= condition.value;\n      case 'lt': return value < condition.value;\n      case 'lte': return value <= condition.value;\n      case 'eq': return value === condition.value;\n      default: return false;\n    }\n  }\n\n  private async createAlert(rule: AlertRule, alertData: {\n    title: string;\n    description: string;\n    context: Record<string, any>;\n    incidentId?: string;\n  }): Promise<void> {\n    // Check cooldown\n    const lastTriggered = this.cooldownTracker.get(rule.id);\n    if (lastTriggered && rule.cooldownMinutes > 0) {\n      const cooldownEnd = new Date(lastTriggered.getTime() + rule.cooldownMinutes * 60 * 1000);\n      if (new Date() < cooldownEnd) {\n        return;\n      }\n    }\n\n    // Check suppression rules\n    if (await this.isAlertSuppressed(rule)) {\n      return;\n    }\n\n    const alertId = `alert_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    const alert: Alert = {\n      id: alertId,\n      ruleId: rule.id,\n      ruleName: rule.name,\n      severity: rule.severity,\n      title: alertData.title,\n      description: alertData.description,\n      triggeredAt: new Date(),\n      status: 'open',\n      context: alertData.context,\n      notifications: [],\n      escalations: [],\n      incidentId: alertData.incidentId\n    };\n\n    this.activeAlerts.set(alertId, alert);\n    this.cooldownTracker.set(rule.id, new Date());\n\n    // Send notifications\n    await this.sendNotifications(alert, rule.channels);\n\n    // Schedule escalations\n    if (rule.escalationRules) {\n      this.scheduleEscalations(alert, rule.escalationRules);\n    }\n\n    // Publish alert event\n    await this.eventPublisher.publish({\n      type: 'alert.triggered',\n      data: alert,\n      timestamp: new Date(),\n      source: 'alerting-system'\n    });\n\n    this.logger.warn('Alert triggered', {\n      alertId,\n      ruleId: rule.id,\n      severity: rule.severity,\n      title: alertData.title\n    });\n  }\n\n  private async isAlertSuppressed(rule: AlertRule): Promise<boolean> {\n    if (!rule.suppressionRules) return false;\n    \n    for (const suppressionRule of rule.suppressionRules) {\n      switch (suppressionRule.type) {\n        case 'time_window':\n          const { startTime, endTime } = suppressionRule.config;\n          const now = new Date();\n          const currentTime = now.getHours() * 60 + now.getMinutes();\n          const start = parseInt(startTime.split(':')[0]) * 60 + parseInt(startTime.split(':')[1]);\n          const end = parseInt(endTime.split(':')[0]) * 60 + parseInt(endTime.split(':')[1]);\n          \n          if (currentTime >= start && currentTime <= end) {\n            return true;\n          }\n          break;\n          \n        case 'maintenance':\n          const maintenanceKey = `maintenance_${rule.id}`;\n          if (this.suppressionStates.get(maintenanceKey)) {\n            return true;\n          }\n          break;\n          \n        case 'dependency':\n          // Check if dependent service is down\n          const { dependentService } = suppressionRule.config;\n          if (this.selfHealingMonitor) {\n            const incidents = this.selfHealingMonitor.getActiveIncidents();\n            const dependentIncident = incidents.find(i => \n              i.context.service === dependentService && i.status === 'open'\n            );\n            if (dependentIncident) {\n              return true;\n            }\n          }\n          break;\n      }\n    }\n    \n    return false;\n  }\n\n  private async sendNotifications(alert: Alert, channels: AlertChannel[]): Promise<void> {\n    for (const channel of channels) {\n      if (!channel.enabled) continue;\n      \n      const notificationId = `notif_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n      \n      const notification: AlertNotification = {\n        id: notificationId,\n        channel,\n        sentAt: new Date(),\n        status: 'pending'\n      };\n      \n      alert.notifications.push(notification);\n      \n      try {\n        await this.sendNotification(alert, channel, notification);\n        notification.status = 'sent';\n      } catch (error) {\n        notification.status = 'failed';\n        notification.error = error.message;\n        \n        this.logger.error('Failed to send alert notification', {\n          alertId: alert.id,\n          channel: channel.type,\n          error: error.message\n        });\n      }\n    }\n  }\n\n  private async sendNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    switch (channel.type) {\n      case 'email':\n        await this.sendEmailNotification(alert, channel, notification);\n        break;\n      case 'slack':\n        await this.sendSlackNotification(alert, channel, notification);\n        break;\n      case 'pagerduty':\n        await this.sendPagerDutyNotification(alert, channel, notification);\n        break;\n      case 'webhook':\n        await this.sendWebhookNotification(alert, channel, notification);\n        break;\n      case 'sms':\n        await this.sendSMSNotification(alert, channel, notification);\n        break;\n      default:\n        throw new Error(`Unsupported notification channel: ${channel.type}`);\n    }\n  }\n\n  private async sendEmailNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    // Implementation would use your email service (SES, SendGrid, etc.)\n    const { recipients, subject } = channel.config;\n    \n    // Placeholder for actual email sending\n    this.logger.info('Email notification sent', {\n      alertId: alert.id,\n      recipients,\n      subject: subject || alert.title\n    });\n  }\n\n  private async sendSlackNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    // Implementation would use Slack webhook or API\n    const { webhook, channel: slackChannel } = channel.config;\n    \n    const payload = {\n      channel: slackChannel,\n      text: alert.title,\n      attachments: [\n        {\n          color: this.getSeverityColor(alert.severity),\n          fields: [\n            {\n              title: 'Severity',\n              value: alert.severity.toUpperCase(),\n              short: true\n            },\n            {\n              title: 'Triggered At',\n              value: alert.triggeredAt.toISOString(),\n              short: true\n            },\n            {\n              title: 'Description',\n              value: alert.description,\n              short: false\n            }\n          ]\n        }\n      ]\n    };\n    \n    // Placeholder for actual Slack webhook call\n    this.logger.info('Slack notification sent', {\n      alertId: alert.id,\n      channel: slackChannel,\n      webhook\n    });\n  }\n\n  private async sendPagerDutyNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    // Implementation would use PagerDuty Events API\n    const { serviceKey } = channel.config;\n    \n    const payload = {\n      service_key: serviceKey,\n      event_type: 'trigger',\n      description: alert.title,\n      details: {\n        severity: alert.severity,\n        context: alert.context,\n        triggered_at: alert.triggeredAt.toISOString()\n      }\n    };\n    \n    // Placeholder for actual PagerDuty API call\n    this.logger.info('PagerDuty notification sent', {\n      alertId: alert.id,\n      serviceKey\n    });\n  }\n\n  private async sendWebhookNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    // Implementation would make HTTP POST to webhook URL\n    const { url, headers } = channel.config;\n    \n    const payload = {\n      alert,\n      timestamp: new Date().toISOString()\n    };\n    \n    // Placeholder for actual webhook call\n    this.logger.info('Webhook notification sent', {\n      alertId: alert.id,\n      url\n    });\n  }\n\n  private async sendSMSNotification(alert: Alert, channel: AlertChannel, notification: AlertNotification): Promise<void> {\n    // Implementation would use SMS service (Twilio, SNS, etc.)\n    const { phoneNumbers } = channel.config;\n    \n    const message = `ALERT: ${alert.title} - ${alert.description}`;\n    \n    // Placeholder for actual SMS sending\n    this.logger.info('SMS notification sent', {\n      alertId: alert.id,\n      phoneNumbers,\n      message\n    });\n  }\n\n  private getSeverityColor(severity: string): string {\n    switch (severity) {\n      case 'critical': return 'danger';\n      case 'high': return 'warning';\n      case 'medium': return 'good';\n      case 'low': return '#439FE0';\n      default: return 'good';\n    }\n  }\n\n  private scheduleEscalations(alert: Alert, escalationRules: EscalationRule[]): void {\n    escalationRules.forEach((rule, index) => {\n      const timerId = setTimeout(async () => {\n        await this.processEscalation(alert, rule, index + 1);\n      }, rule.delayMinutes * 60 * 1000);\n      \n      this.escalationTimers.set(`${alert.id}_${index}`, timerId);\n    });\n  }\n\n  private async processEscalation(alert: Alert, rule: EscalationRule, level: number): Promise<void> {\n    // Check if alert is still open and escalation condition is met\n    if (alert.status !== 'open') {\n      return;\n    }\n    \n    if (rule.condition === 'worsening') {\n      // Check if the situation has worsened\n      // This would require additional logic to determine worsening conditions\n    }\n    \n    const escalation: AlertEscalation = {\n      level,\n      triggeredAt: new Date(),\n      channels: rule.channels,\n      notifications: []\n    };\n    \n    alert.escalations.push(escalation);\n    \n    // Send escalation notifications\n    for (const channel of rule.channels) {\n      if (!channel.enabled) continue;\n      \n      const notificationId = `escalation_notif_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n      \n      const notification: AlertNotification = {\n        id: notificationId,\n        channel,\n        sentAt: new Date(),\n        status: 'pending'\n      };\n      \n      escalation.notifications.push(notification);\n      \n      try {\n        // Modify alert title to indicate escalation\n        const escalatedAlert = {\n          ...alert,\n          title: `ESCALATED (Level ${level}): ${alert.title}`,\n          description: `${alert.description}\\n\\nThis alert has been escalated due to lack of resolution.`\n        };\n        \n        await this.sendNotification(escalatedAlert, channel, notification);\n        notification.status = 'sent';\n      } catch (error) {\n        notification.status = 'failed';\n        notification.error = error.message;\n      }\n    }\n    \n    this.logger.warn('Alert escalated', {\n      alertId: alert.id,\n      level,\n      channels: rule.channels.length\n    });\n  }\n\n  private async processEscalations(): Promise<void> {\n    // This method is called periodically to handle any missed escalations\n    // Most escalations are handled by scheduled timers, but this provides a backup\n  }\n\n  private async cleanupResolvedAlerts(): Promise<void> {\n    const cutoffTime = new Date(Date.now() - 24 * 60 * 60 * 1000); // 24 hours ago\n    \n    for (const [alertId, alert] of this.activeAlerts) {\n      if (alert.status === 'resolved' && alert.resolvedAt && alert.resolvedAt < cutoffTime) {\n        this.activeAlerts.delete(alertId);\n        \n        // Clear any pending escalation timers\n        for (const [timerId, timer] of this.escalationTimers) {\n          if (timerId.startsWith(alertId)) {\n            clearTimeout(timer);\n            this.escalationTimers.delete(timerId);\n          }\n        }\n      }\n    }\n  }\n\n  // Public API methods\n  public async resolveAlert(alertId: string, resolution: string): Promise<void> {\n    const alert = this.activeAlerts.get(alertId);\n    if (!alert) {\n      throw new Error(`Alert ${alertId} not found`);\n    }\n    \n    alert.status = 'resolved';\n    alert.resolvedAt = new Date();\n    \n    // Clear escalation timers\n    for (const [timerId, timer] of this.escalationTimers) {\n      if (timerId.startsWith(alertId)) {\n        clearTimeout(timer);\n        this.escalationTimers.delete(timerId);\n      }\n    }\n    \n    await this.eventPublisher.publish({\n      type: 'alert.resolved',\n      data: { alert, resolution },\n      timestamp: new Date(),\n      source: 'alerting-system'\n    });\n    \n    this.logger.info('Alert resolved', {\n      alertId,\n      resolution\n    });\n  }\n\n  public async acknowledgeAlert(alertId: string, acknowledgedBy: string): Promise<void> {\n    const alert = this.activeAlerts.get(alertId);\n    if (!alert) {\n      throw new Error(`Alert ${alertId} not found`);\n    }\n    \n    alert.status = 'acknowledged';\n    alert.context.acknowledgedBy = acknowledgedBy;\n    alert.context.acknowledgedAt = new Date().toISOString();\n    \n    await this.eventPublisher.publish({\n      type: 'alert.acknowledged',\n      data: { alert, acknowledgedBy },\n      timestamp: new Date(),\n      source: 'alerting-system'\n    });\n    \n    this.logger.info('Alert acknowledged', {\n      alertId,\n      acknowledgedBy\n    });\n  }\n\n  public addAlertRule(rule: AlertRule): void {\n    this.alertRules.set(rule.id, rule);\n    this.logger.info('Alert rule added', { ruleId: rule.id, ruleName: rule.name });\n  }\n\n  public removeAlertRule(ruleId: string): void {\n    this.alertRules.delete(ruleId);\n    this.logger.info('Alert rule removed', { ruleId });\n  }\n\n  public getActiveAlerts(): Alert[] {\n    return Array.from(this.activeAlerts.values());\n  }\n\n  public getAlert(alertId: string): Alert | undefined {\n    return this.activeAlerts.get(alertId);\n  }\n\n  public enableMaintenanceMode(ruleId: string): void {\n    const maintenanceKey = `maintenance_${ruleId}`;\n    this.suppressionStates.set(maintenanceKey, true);\n    this.logger.info('Maintenance mode enabled', { ruleId });\n  }\n\n  public disableMaintenanceMode(ruleId: string): void {\n    const maintenanceKey = `maintenance_${ruleId}`;\n    this.suppressionStates.delete(maintenanceKey);\n    this.logger.info('Maintenance mode disabled', { ruleId });\n  }\n\n  public getMetrics(): any {\n    return {\n      activeAlerts: this.activeAlerts.size,\n      alertRules: this.alertRules.size,\n      escalationTimers: this.escalationTimers.size,\n      suppressionStates: this.suppressionStates.size,\n      alertsByStatus: {\n        open: Array.from(this.activeAlerts.values()).filter(a => a.status === 'open').length,\n        acknowledged: Array.from(this.activeAlerts.values()).filter(a => a.status === 'acknowledged').length,\n        resolved: Array.from(this.activeAlerts.values()).filter(a => a.status === 'resolved').length\n      },\n      alertsBySeverity: {\n        critical: Array.from(this.activeAlerts.values()).filter(a => a.severity === 'critical').length,\n        high: Array.from(this.activeAlerts.values()).filter(a => a.severity === 'high').length,\n        medium: Array.from(this.activeAlerts.values()).filter(a => a.severity === 'medium').length,\n        low: Array.from(this.activeAlerts.values()).filter(a => a.severity === 'low').length\n      }\n    };\n  }\n}"